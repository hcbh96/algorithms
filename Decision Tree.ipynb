{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decison Tree\n",
    "\n",
    "Decision trees are formed from very simple components. Decision trees are particularly popular because it can be very easy to understand exactly why a specific prediction was made.\n",
    "\n",
    "Decision tree work by recusively splitting an input dataset into two subgroups. The split is decided using some metric which is comparable to a loss function in other settings. Metrics used to decide on the split include Gini impurity, Information Gain, Variance Reduction and measure of goodness. \n",
    "\n",
    "Decision trees are often referred to as Classification And Regression Trees (CART) which was introduced by Brieman in 1984. \n",
    "\n",
    "CART form the underlying components of ensemble methods such as Random Forests, AdaBoost and Rotation Forests.\n",
    "\n",
    "Notable decision tree algorihtms include:\n",
    "* CART - Nonparametric decsion tree learning technique that produce a classification or regression dependent on the input variable.\n",
    "* Chi-squared automatic interation and detection - Often used in the context of direct marketing to select groups of consumers and predict how their reponses to some variables affect some other variables.\n",
    "* MARS - Multivariate Adaptive Regression Splines are a from of regression analysis that work by fitting piece wise linear regressions. They are capable of automatically modeling nonlinearities. The algorithm is computationally very expensive.\n",
    "\n",
    "\n",
    "For the purposes of these notes we are going to focus on CART.\n",
    "\n",
    "## Mathematics\n",
    "\n",
    "CART works by recursively partitioning the training set $T$. The aim is to find a partition of the training set that minimises a loss function $L$. Each node in the tree is associated with the generation of a particular subsets $T_i \\subset T = \\{(x,y)\\}^k_{n=1}$, where $x$ is an vector of independent variables and $y$ is the the dependent variable. \n",
    "\n",
    "The partition is then defined by a function capable of splitting the data according to the value of a particular variable from the input dataset $x_i \\in T$.\n",
    "\n",
    "Considering a feature $j$ from an input set $T$ and taking $a$ as an arbitraty value then a split can be defined by two subsets:\n",
    "\n",
    "$$T_l = \\{t \\in T : x_j \\leq A\\}$$\n",
    "\n",
    "and \n",
    "\n",
    "$$T_r = \\{t\\in T : x_j \\geq A\\}$$\n",
    "\n",
    "A categorical feature can be split by using \n",
    "\n",
    "$$T_l = \\{t\\in T : x_j = A\\}$$\n",
    "\n",
    "and \n",
    "\n",
    "$$T_r = \\{t\\in T : x_j \\neq A\\}$$\n",
    "\n",
    "When partitoning a dataset the decision tree takes into account all possible paritions, it tests each partition and selects the one that minimises the defined loss function $L$. \n",
    "\n",
    "The loss function $L$ used to measure compare the value of different splits tends to be different for continuous and categorical variables, and their exist many loss functions that can be used in both cases. In the continuous setting a square loss can be calculated for each subset using:\n",
    "\n",
    "$$\n",
    "L = \\sum_{t \\in T}(y_t - f(x_t))^2\n",
    "$$\n",
    "\n",
    "There are many other continuous loss functions that could be used in place of an L2 loss.\n",
    "\n",
    "In the case of CART the Gini impurity is used which provides an estimate on how pure (homogeneous) a subgroup is. The Gini impurity is calculated using the the probability of selecting a sample that corresponds to given class $c_i \\in C$ for the sample in the split\n",
    "\n",
    "$$L = \\sum_{c_i \\in C}p(c_i)(1-p(c_i))$$\n",
    "\n",
    "This leads to an impurity of 0 when all measures of the same class and increases as the homogeneity of the class decreases.\n",
    "\n",
    "The process of partitioning continues for each generated node until some stopping criteria has been met. The final tree is then returned and can be used for prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python\n",
    "\n",
    "In this section we will break down decision trees into the various components. In this function we treat the decision tree as a function that creates a tree by recursively splitting nodes until the model has been trained. The model includes several parts, the first is initialisation of a node which is given a subset of data. Once the node has been split we then find the optimal variable to split the node on `find_varsplit`. During the `find_varsplit` method we use `find_better_split` which assess the efficacy of the current split vs the current best split. This assessment is made on the value of the a equivalent to the RMSE which can be seen as a loss function. Once the best split has been found it is returned and the data is split between the `lhs` and `rhs`, which both initialise their own nodes. This recursion happens again until the stopping criterion is met."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "# Fit the model given training data and parameters\n",
    "def fit(self, X, y, min_leaf = 5):\n",
    "    self.dtree = Node(X, y, np.array(np.arange(len(y))), min_leaf)\n",
    "    return self\n",
    "\n",
    "# Make a prediction\n",
    "def predict(self, X):\n",
    "    return self.dtree.predict(X.values)\n",
    "\n",
    "# Node constructor\n",
    "class Node:\n",
    "    # Initialise the necessary aspects of a node\n",
    "    def __init__(self, x, y, idxs, min_leaf=5):\n",
    "        self.x = x \n",
    "        self.y = y\n",
    "        self.idxs = idxs \n",
    "        self.min_leaf = min_leaf\n",
    "        self.row_count = len(idxs)\n",
    "        self.col_count = x.shape[1]\n",
    "        self.val = np.mean(y[idxs])\n",
    "        self.score = float('inf')\n",
    "        self.find_varsplit()\n",
    "        \n",
    "    # Find the correct splitpoint\n",
    "    def find_varsplit(self):\n",
    "        # Go through each of the features looking for best split\n",
    "        for c in range(self.col_count): \n",
    "            self.find_better_split(c)\n",
    "        # Return without action if leaf is found\n",
    "        if self.is_leaf: return\n",
    "        # Split the values into lhs and rhs and recurse\n",
    "        x = self.split_col\n",
    "        lhs = np.nonzero(x <= self.split)[0]\n",
    "        rhs = np.nonzero(x > self.split)[0]\n",
    "        self.lhs = Node(self.x, self.y, self.idxs[lhs], self.min_leaf)\n",
    "        self.rhs = Node(self.x, self.y, self.idxs[rhs], self.min_leaf)\n",
    "    \n",
    "    # Using the variable index to assess split on feature\n",
    "    def find_better_split(self, var_idx):\n",
    "          \n",
    "        # Get all the values from the index in question    \n",
    "        x = self.x.values[self.idxs, var_idx]\n",
    "        \n",
    "        # Append through the rows of the input matrix and split on the defined value\n",
    "        for r in range(self.row_count):\n",
    "            lhs = x <= x[r]\n",
    "            rhs = x > x[r]\n",
    "            \n",
    "            # Early check to see if there are too few samples in the leaf\n",
    "            if rhs.sum() < self.min_leaf or lhs.sum() < self.min_leaf: continue\n",
    "            \n",
    "            # Get the score for the current split\n",
    "            curr_score = self.find_score(lhs, rhs)\n",
    "            # Replace best split if better\n",
    "            if curr_score < self.score: \n",
    "                self.var_idx = var_idx\n",
    "                self.score = curr_score\n",
    "                self.split = x[r]\n",
    "                \n",
    "    # Calculate the score of the split this is equivalent to the rmse\n",
    "    def find_score(self, lhs, rhs):\n",
    "        y = self.y[self.idxs]\n",
    "        lhs_std = y[lhs].std()\n",
    "        rhs_std = y[rhs].std()\n",
    "        return lhs_std * lhs.sum() + rhs_std * rhs.sum()\n",
    "                \n",
    "    @property\n",
    "    def split_col(self): \n",
    "        return self.x.values[self.idxs,self.var_idx]\n",
    "                \n",
    "    @property\n",
    "    def is_leaf(self): \n",
    "        return self.score == float('inf')                \n",
    "\n",
    "    \n",
    "    def predict(self, x):\n",
    "        return np.array([self.predict_row(xi) for xi in x])\n",
    "\n",
    "    def predict_row(self, xi):\n",
    "        if self.is_leaf: return self.val\n",
    "        node = self.lhs if xi[self.var_idx] <= self.split else self.rhs\n",
    "        return node.predict_row(xi)\n",
    " \n",
    "\n",
    "# Import training dataset\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_diabetes\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "data = load_diabetes()\n",
    "\n",
    "X_train = pd.DataFrame(data=data.data)\n",
    "y_train = pd.DataFrame(data=data.target).iloc[:,0]\n",
    "\n",
    "\n",
    "regressor = DecisionTreeRegressor().fit(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
