{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "In this tutorial we will be going through logistic regression. The aim is to acheive the following in this tutorial. Provide a situation in which logistic regression is a useful algorithm. Provide a mathematical perspective on Logistic Regression. Apply Logistic regression to a sample dataset to show how it can be implemented in Python, in R and in Julia.\n",
    "\n",
    "### Estimating probability\n",
    "Logistic regression is used to model for the purposes of classification. \n",
    "\n",
    "Imagine we want to predict the probability that a student will pass an exam based on their test marks across the year. Logistic regression provides us with a tool to turn a set of input features into a probability measure.\n",
    "\n",
    "The probably of the response is conditional on a set of variables which can themselves be either continuous or discreet.\n",
    "\n",
    "Logistic regression works in a very similar manner to linear regression but the prediction is passed through a sigmoid function which converts the linear function to a sigmoid function. A Sigmoid function is defined over the space 0 to 1.\n",
    "\n",
    "## Mathematics\n",
    "Logistic regression finds its basis in the logit function\n",
    "$$\n",
    " \\mathcal{l} = log_b \\frac{p}{1-p}\n",
    "$$\n",
    "\n",
    "Conider the case in which we have a set of predictors $x = [x_1, \\dots, x_n]$ and a binary response $y$. We denote the probability of $P(y=1)=p$. Assume a linear relationship between the predictors governed by the weights $\\theta = [\\theta_1, \\dots, \\theta_n] and the logit function\n",
    "\n",
    "$$\n",
    "    \\mathcal{l} = log_b \\frac{p}{1-p} = \\theta \\cdot x\n",
    "$$\n",
    "\n",
    "The odds $p$ can be recovered by taking exponentiating the logit function:\n",
    "\n",
    "$$\n",
    "\\frac{p}{1-p} = b^{\\theta^T \\cdot x}\n",
    "$$\n",
    "\n",
    "By simple algerbra we can extract the probability\n",
    "\n",
    "$$p = \\frac{b^{\\theta^T \\cdot x}}{b^{\\theta^T \\cdot x} + 1} = \\frac{1}{1 + b^{\\theta^T \\cdot x}} = S_b(\\theta^T \\cdot x)$$\n",
    "\n",
    "Where $S_b$ is the sigmoid function (inverse of the logit) with base $b$. This means that once $\\theta$ is fixed we can easily compute the liklihood of the response. The base $b$ in most applications is usually taken to be $e$, however it can from time to time be easier to work in other bases.\n",
    "\n",
    "### Finding Beta\n",
    "\n",
    "Logistic regression concerns itself with finding feature weights $\\theta$ that maximise the probability of observations in a training set. Consider a generealised linear model \n",
    "\n",
    "$$\n",
    "h_\\theta(X) = \\frac{1}{1+e^{-\\theta \\cdot X}} = Pr(Y=1|X;\\theta)\n",
    "$$\n",
    "\n",
    "This therfore returns some value $ 0 \\geq h_\\theta(X) \\leq 1$. Since $Y \\in \\{0,1\\}$, $Pr(y|X;\\theta)=h_{\\theta}(X)^y(1-h_{\\theta}(X))^{(1-y)}$. Given this loss a liklihood function can be calculated under the assumption that all observations in the sample are independently distributed.\n",
    "\n",
    "$$L(\\theta| y;x) = Pr(Y| X; \\theta) = \\prod_iPr(y_i|x_i;\\theta) = \\prod_ih_{\\theta}(x_i)^{y_i}(1-h_{\\theta}(x_i))^{(1-y)}$$\n",
    "\n",
    "Breaking this down consider the case where the true label $y=0$ then $(1-h_{\\theta}(x))^{(1-y)} = 1$ leaving $h_{\\theta}(x)^y$, therefore in this case we want the probability to be high, now consider if the true label is false and you will see the opposite. Therefore we want to find the parameters $\\theta$ that maximise this liklihood function. This can be done using gradient descent.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Components\n",
    "In this section we will build the necessary components that will allow us to perform Logistic Regression. We will then analyse the various componenets on top of which logistic regression is based.\n",
    "\n",
    "### Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Cost:  394.40074573860846\n",
      "Optimised Cost:  0.04099637846897899\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.optimize import fmin_tnc\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "data = load_breast_cancer()\n",
    "\n",
    "X = data.data\n",
    "y = data.target\n",
    "theta = np.zeros((X.shape[1], 1))\n",
    "\n",
    "# Define the model we are using\n",
    "\n",
    "def sigmoid(x):\n",
    "    # Activation function used to map any real value between 0 and 1\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def net_input(theta, x):\n",
    "    # Computes the weighted sum of inputs\n",
    "    return np.dot(x, theta)\n",
    "\n",
    "\n",
    "def probability(theta, x):\n",
    "    # Returns the probability after passing through sigmoid\n",
    "    return sigmoid(net_input(theta, x))\n",
    "\n",
    "\n",
    "# Define the cost function\n",
    "\n",
    "def cost_function(theta, x, y):\n",
    "    # Computes the cost function for all the training samples\n",
    "    m = x.shape[0]\n",
    "    total_cost = -(1 / m) * np.sum(\n",
    "        y * np.log(probability(theta, x)) + (1 - y) * np.log(\n",
    "            1 - probability(theta, x)))\n",
    "    return total_cost\n",
    "\n",
    "\n",
    "# Define the optimisation method\n",
    "\n",
    "def gradient(theta, x, y):\n",
    "    # Computes the gradient of the cost function at the point theta\n",
    "    m = x.shape[0]\n",
    "    return (1 / m) * np.dot(x.T, sigmoid(net_input(theta,   x)) - y)\n",
    "\n",
    "\n",
    "def fit(x, y, theta):\n",
    "    opt_weights = fmin_tnc(func=cost_function, x0=theta,\n",
    "                  fprime=gradient,args=(x, y.flatten()))\n",
    "    return opt_weights[0]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "parameters = fit(X, y, theta)\n",
    "\n",
    "print(\"Initial Cost: \", cost_function(theta, X, y))\n",
    "print(\"Optimised Cost: \", cost_function(parameters, X, y))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
